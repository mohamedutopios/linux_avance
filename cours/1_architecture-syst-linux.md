Je vais préparer une explication détaillée de chaque point de l'architecture système Linux avec des exemples concrets et des comparaisons. J’inclurai également des schémas ou illustrations pour mieux visualiser les concepts abordés.

# Architecture du système Linux

## 1. Vue d’ensemble 
 ([Kernel Architecture of Linux - Scaler Topics](https://www.scaler.com/topics/linux-kernel-architecture/)) *Schéma illustrant l’architecture en couches d’un système Linux, avec les applications en espace utilisateur reposant sur les services du noyau Linux, lui-même au-dessus du matériel physique.* Le système Linux est structuré en couches : en bas, le **matériel** (processeur, mémoire, périphériques) ; au-dessus, le **noyau** du système (le kernel) qui s’exécute en mode privilégié ; et tout en haut, l’**espace utilisateur** où tournent les applications et processus utilisateur. Cette séparation entre espace noyau et espace utilisateur est fondamentale pour la stabilité et la sécurité : le noyau dispose d’un accès direct au matériel et gère les ressources, tandis que les programmes utilisateurs doivent passer par des appels contrôlés (appels système) pour solliciter ses services. Cela offre une protection mémoire et empêche qu’un programme en espace utilisateur ne corrompe directement le noyau ou les autres processus ([User space and kernel space - Wikipedia](https://en.wikipedia.org/wiki/User_space_and_kernel_space#:~:text=A%20modern%20computer%20operating%20system,malicious%20or%20errant%20software%20behaviour)). En pratique, lorsqu’une application veut accéder au disque, à la réseau ou à tout périphérique, elle effectue un appel système pour demander au noyau d’exécuter cette opération privilégiée. Le noyau joue donc le rôle d’intermédiaire entre le matériel et les logiciels : il gère la **mémoire**, planifie les **processus**, fournit des abstractions comme les **systèmes de fichiers** et les **interfaces réseau**, et expose ces fonctionnalités aux applications via une interface bien définie.

## 2. Anneaux de protection (rings -1, 0 et 3) 
 ([Fichier:Most common protection rings.svg — Wikilivres](https://fr.m.wikibooks.org/wiki/Fichier:Most_common_protection_rings.svg)) *Représentation simplifiée des anneaux de privilège sur une architecture type x86. L’anneau 0 (cercle central rouge) correspond au code noyau et aux drivers, hautement privilégiés, tandis que l’anneau 3 (zone verte) correspond aux applications utilisateur avec le moins de privilèges.* Sur les processeurs x86, le mode de protection par **anneaux** définit plusieurs niveaux de privilège matériel. L’anneau le plus privilégié est le **ring 0**, utilisé par le noyau (mode superviseur), et l’anneau le moins privilégié est généralement le **ring 3**, utilisé par les applications en espace utilisateur (mode utilisateur). En théorie, x86 offre 4 anneaux (0 à 3), mais la plupart des OS (Unix, Windows…) n’en utilisent que deux : ring 0 pour le noyau et ring 3 pour l’espace utilisateur ([Protection ring - Wikipedia](https://en.wikipedia.org/wiki/Protection_ring#:~:text=ImageWhile%20x86%20has%204%20protection,use%20ring%200%20and%203)). Linux suit ce modèle, laissant les anneaux intermédiaires inutilisés. Cette séparation garantit qu’un programme en ring 3 ne peut pas exécuter d’instructions sensibles ni accéder directement au matériel sans passer par le ring 0 – toute tentative illégitime déclencherait une faute de protection générale interceptée par le noyau ([Protection ring - Wikipedia](https://en.wikipedia.org/wiki/Protection_ring#:~:text=A%20privilege%20level%20in%20the,is%20reported%20to%20the%20OS)) ([Protection ring - Wikipedia](https://en.wikipedia.org/wiki/Protection_ring#:~:text=The%20hardware%20severely%20restricts%20the,In%20addition%2C%20the)). 

Les **hyperviseurs** (virtualisation) exploitent un niveau de privilège encore supérieur, parfois qualifié d’**anneau -1**. En effet, les extensions de virtualisation matérielle (Intel VT-x, AMD-V) introduisent un mode d’exécution spécial pour l’hyperviseur, plus privilégié que le noyau lui-même ([Protection ring - Wikipedia](https://en.wikipedia.org/wiki/Protection_ring#:~:text=In%20x86%20systems%2C%20the%20x86,23)). Dans ce mode, l’hyperviseur (comme KVM ou Xen) s’exécute en *ring -1* et peut contrôler entièrement la machine, tandis que chaque système d’exploitation invité tourne en ring 0 virtuel sous son contrôle. Avant l’apparition de ces extensions, un hyperviseur devait ruser : par exemple, Xen exécutait les noyaux invités en ring 1 et interceptait via un mécanisme *trap-and-emulate* toutes les instructions privilégiées non autorisées ([Protection ring - Wikipedia](https://en.wikipedia.org/wiki/Protection_ring#:~:text=Before%20hardware,Trap%20and%20Emulate)). Désormais, grâce au *“mode root”* VT-x, l’hyperviseur dispose d’un niveau dédié (ring -1) pour arbitrer les accès, et les OS invités peuvent fonctionner en ring 0 sans compromettre l’isolement ([Virtualization and Ring Negative One](https://blog.codinghorror.com/virtualization-and-ring-negative-one/#:~:text=,0%20%E2%80%93%20without%20software%20emulation)). En résumé, l’anneau 0 correspond au noyau Linux (et drivers) s’exécutant en mode superviseur, l’anneau 3 correspond aux programmes utilisateurs en mode non privilégié, et l’anneau -1 désigne le mode hyperviseur offert par la virtualisation matérielle pour héberger des machines virtuelles avec un niveau de privilège supérieur au noyau hôte.

## 3. Plateformes matérielles supportées 
Une grande force du noyau Linux est sa **portabilité** : il a été porté sur un très large éventail d’architectures matérielles. Historiquement né sur PC Intel 80386, Linux prend aujourd’hui en charge des **douzaines d’architectures** différentes ([What computer architectures are available? - Ask Ubuntu](https://askubuntu.com/questions/10811/what-computer-architectures-are-available#:~:text=The%20linux%20kernel%20is%20ported,libc%2C%20gcc%2C%20etc)). Les principales plateformes sont :
- **x86 32 bits (IA-32)** et **x86 64 bits (x86_64/AMD64)** – architectures PC grand public, serveurs et laptops.
- **ARM** (32 bits ARMv7 et 64 bits ARMv8/AArch64) – omniprésent dans l’embarqué, les smartphones, tablettes et micro-ordinateurs (Raspberry Pi, etc.).
- **RISC-V** – une architecture RISC ouverte et modulaire, dont le support a été intégré au noyau Linux en 2022 ([RISC-V - Wikipedia](https://en.wikipedia.org/wiki/RISC-V#:~:text=Mainline%20support%20for%20RISC,15)).
- **PowerPC/POWER** – architecture RISC d’IBM utilisée sur des stations de travail, serveurs et anciens Mac (Linux tourne sur des systèmes IBM Power et sur d’anciennes consoles de jeu, par ex. la PS3, basées sur PowerPC).
- **MIPS** – architecture RISC autrefois répandue dans les systèmes embarqués et stations SGI, supportée par Linux.
- **SPARC** – architecture RISC de Sun/Oracle (stations Unix), également supportée par Linux.
- **IBM S/390 (s390x)** – l’architecture des grands mainframes IBM, que Linux supporte nativement depuis les années 2000.
- Et bien d’autres : **Microblaze**, **MIPS64**, **SuperH**, **Alpha**, **PA-RISC**, **Itanium**, **ARC**, etc. (toutes plus ou moins supportées selon les versions). 

Par exemple, une distribution comme Ubuntu fournit des images officielles pour x86_64, ARM64, PowerPC64 et IBM System z ([SupportedArchitectures - Community Help Wiki](https://help.ubuntu.com/community/SupportedArchitectures#:~:text=Ubuntu%20is%20currently%20officially%20compatible,aka%20S)), ce qui illustre la diversité du matériel pris en charge. Linux peut ainsi fonctionner aussi bien sur un **microcontrôleur** de quelques MHz avec peu de mémoire que sur un **supercalculateur** massif – ce qui témoigne de son extrême adaptabilité. Cette portabilité est rendue possible par l’abstraction que fait le noyau du matériel : une grande partie du code est indépendante de l’architecture, et seules quelques couches (gestion du CPU, interruptions, etc.) sont spécifiques à chaque architecture, souvent isolées dans des sous-répertoires du code source (par ex. `arch/x86`, `arch/arm`…). Le support d’une nouvelle architecture consiste à implémenter ces couches d’adaptation. 

## 4. Noyau Linux et LKM (Loadable Kernel Modules) 
Le noyau Linux adopte une architecture dite **monolithique modulaire**. *Monolithique* signifie que le noyau constitue un seul bloc de code s’exécutant en espace noyau (par opposition à un micronoyau qui éclaterait les services en plusieurs processus séparés). *Modulaire* signifie que ce noyau peut être étendu à chaud via des **modules chargeables**. En effet, Linux est conçu de manière modulaire, permettant d’intégrer des composants du noyau sous forme de modules logiciels qu’on peut charger ou décharger dynamiquement selon les besoins ([Linux kernel - Wikipedia](https://en.wikipedia.org/wiki/Linux_kernel#:~:text=a%20modular%20design%20such%20that,OS%20runs%20in%20kernel%20space)). À la compilation du noyau, de nombreux drivers et fonctionnalités peuvent être sélectionnés soit comme faisant partie intégrante du noyau, soit comme modules séparés (.ko). 

Les **LKM (Loadable Kernel Modules)** sont ces modules du noyau que l’on peut insérer ou retirer à l’exécution. Ils offrent plusieurs avantages : ajouter un module permet d’activer une nouvelle fonctionnalité (par exemple le support d’un nouveau système de fichiers ou d’un périphérique) sans recompiler ni redémarrer le noyau ; retirer un module libère les ressources associées si le matériel n’est plus utilisé. Cela contribue à réduire la taille du noyau en mémoire (on ne charge que les composants nécessaires) et facilite les mises à jour (on peut remplacer un module par une nouvelle version sans interrompre tout le système). Par exemple, les pilotes de certaines cartes réseau, imprimantes ou systèmes de fichiers (ex: **ext4**, **XFS**, **NTFS**…) sont souvent fournis sous forme de modules que l’on insère au besoin. 

Techniquement, un module est un morceau de code compilé séparément (fichier .ko) qui peut s’insérer dans l’espace noyau. Le noyau garde une table des symboles exportés auxquels les modules peuvent faire référence. Lorsqu’on charge un module (avec `insmod` ou `modprobe`), le kernel réalise un lien à chaud : il résout les dépendances du module envers les symboles du noyau ou d’autres modules déjà chargés, alloue de la mémoire kernel pour ce module et y transfère l’exécution. Une fois chargé, le module s’exécute avec les mêmes privilèges que le noyau lui-même. Linux fournit des APIs internes pour écrire des modules, par exemple pour enregistrer un nouveau pilote, un protocole réseau, etc., via des fonctions d’initialisation qui sont appelées au chargement. Inversement, lorsqu’on enlève un module (`rmmod`), le kernel appelle la routine de nettoyage du module puis libère ses ressources. Notons qu’un module mal programmé (buggé) peut potentiellement planter le système autant qu’un bug dans le noyau monolithique, puisqu’il tourne au même niveau de privilège.

En résumé, Linux combine le meilleur des deux approches : un **noyau monolithique** performant où tout le code tourne en mode superviseur, et une **extensibilité modulaire** permettant une grande flexibilité d’utilisation. La plupart des distributions livrent un noyau générique comportant un minimum de fonctionnalités en dur, et tout le reste en modules (fichiers situés sous `/lib/modules/` correspondant à la version du noyau). Au démarrage, seul le noyau de base est chargé ; puis les modules nécessaires (pilotes, etc.) sont insérés à la volée (souvent automatiquement via udev ou *scripts* de démarrage) en fonction du matériel détecté ou des besoins du système ([Lsmod Command in Linux (List Kernel Modules) | Linuxize](https://linuxize.com/post/lsmod-command-in-linux/#:~:text=The%20Linux%20kernel%20has%20a,need%20to%20reboot%20the%20system)). Cette conception facilite également le développement : les programmeurs peuvent ajouter de nouveaux drivers sans avoir à recompiler un noyau entier, et tester/recharger leurs modules de manière itérative.

## 5. Le système de fichiers *root* 
Sous Linux, l’ensemble du système de fichiers s’organise autour d’une racine unique notée `/` (le *root filesystem*). Cette arborescence respecte la norme **FHS (Filesystem Hierarchy Standard)**, un standard qui définit de manière cohérente les emplacements des fichiers et répertoires dans les systèmes de type Unix ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=The%20Filesystem%20Hierarchy%20Standard%20,2)). Sous la racine, on trouve des répertoires bien connus, chacun ayant un rôle spécifique. Voici les principaux répertoires du système et leur fonction :

- **`/` (racine)** : point de départ de la hiérarchie, contient tous les répertoires du système de fichiers Linux ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,wide%20configuration%20files)). C’est le répertoire parent de tout autre chemin.
- **`/bin`** : *binaries* essentiels en mode utilisateur. On y trouve les commandes exécutables indispensables au fonctionnement du système, utilisables par tous les utilisateurs (exemples : `ls`, `cp`, `bash`). Ces programmes sont disponibles même si aucune autre partition n’est montée, ce qui les rend accessibles en mode mono-utilisateur (maintenance) ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,wide%20configuration%20files)).
- **`/sbin`** : *system binaries*. Similaire à `/bin` mais pour les utilitaires système essentiels réservés à l’administrateur (exemples : `fsck` pour vérifier un système de fichiers, `ifconfig`/`ip`, `init`). Historiquement, `/sbin` est pour les binaires d’administration du système.
- **`/etc`** : fichiers de **configuration** du système (configurations globales et spécifiques à la machine) ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=%60%2Fetc%60%20Host)). Par exemple, `/etc/fstab` (table des systèmes de fichiers), `/etc/passwd` (comptes utilisateurs), fichiers de configuration de services, scripts d’initialisation… Ce répertoire contient typiquement des fichiers texte modifiables par l’administrateur pour configurer le comportement du système et des applications.
- **`/home`** : répertoires **personnels** des utilisateurs non privilégiés ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=XML%20.%20,code%20format%2C%20such%20as%20systems)). Chaque utilisateur a son dossier `/home/<nom>` où se trouvent ses fichiers, préférences et données personnelles. Par exemple `/home/alice/Documents`, etc. Ce répertoire permet de séparer les données utilisateur du reste du système.
- **`/root`** : répertoire personnel de l’utilisateur **root** (administrateur du système). Contrairement aux autres homes qui sont sous `/home`, celui du super-utilisateur est directement sous la racine (afin d’être disponible même si `/home` est sur une partition séparée non montée) ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=information%20as%20files,the%20beginning%20of%20the%20boot)).
- **`/lib`** : *libraries* essentielles partagées. Contient les bibliothèques dynamiques (fichiers `.so`) nécessaires aux exécutables de `/bin` et `/sbin` ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=etc.%20,exist%2C%20they%20have%20some%20requirements)). Par exemple, la libc (`libc.so.6`), ld-linux, etc., dont dépendent les commandes de base. Sur les systèmes 64 bits, on trouve souvent `/lib64` pour les librairies 64 bits et `/lib` pour les 32 bits, ou bien `/lib` est un lien symbolique vers `/usr/lib` (selon l’organisation de la distribution).
- **`/usr`** : hiérarchie *secondaire* pour les données *utilisateur* partagées (read-only). C’est souvent le plus gros arborescence : on y installe la majorité des logiciels et données qui ne sont pas propres à un utilisateur unique. Par convention, `/usr` est monté en lecture seule (et potentiellement partagé entre machines) ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=%60%2Fusr%60%20Secondary%20hierarchy%20for%20read,user)). On y trouve notamment :
  - `/usr/bin` (binaries pour les utilisateurs, non essentiels à la récupération du système – ex : éditeurs de texte, compilateurs…),
  - `/usr/sbin` (binaries d’administration non critiques – ex : `apache2`, outils réseau),
  - `/usr/lib` (librairies pour les programmes de `/usr/bin` et `/usr/sbin`),
  - `/usr/include` (fichiers d’en-tête pour le développement),
  - `/usr/share` (données indépendantes de l’architecture, ex : pages de manuels, locales, icônes),
  - `/usr/src` (codes sources, par ex. code source du noyau dans `/usr/src/linux`),
  - `/usr/local` (hiérarchie pour les installations locales de l’administrateur, afin de ne pas écraser celles de la distribution).
- **`/var`** : données *variables* du système ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,logs%2C%20spool%20files%2C%20and%20temporary)). Ce répertoire contient les fichiers dont le contenu est amené à changer fréquemment pendant l’exploitation du système. Par exemple : les logs dans `/var/log` (fichiers journaux du système et des applications), les spool d’impression dans `/var/spool`, les bases de données (certains SGBD par défaut dans `/var/lib`), les fichiers de lock (`/var/lock`), les files d’attente de mail (`/var/mail`), etc. La séparation `/var` permet d’éviter que ces fichiers variables (potentiellement volumineux) n’emplissent la partition racine.
- **`/tmp`** : répertoire pour les fichiers **temporaires**. Accessible en écriture à tous les utilisateurs, il stocke des données provisoires (souvent nettoyé à chaque redémarrage). Par exemple, les applications y déposent des fichiers temporaires lors de leur exécution. Il peut être monté en tmpfs (en RAM) sur certains systèmes pour accélérer les I/O temporaires.
- **`/dev`** : conteneur des *device files*, fichiers spéciaux représentant les périphériques du système ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=%28e,wide%20%2092)). Dans Unix/Linux, quasiment tout périphérique est accessible via un fichier dans `/dev`. Par exemple, `/dev/sda` représente un disque dur, `/dev/tty1` une console, `/dev/null` un puits sans fond, etc. Ces fichiers permettent aux programmes d’interagir avec les périphériques en utilisant les appels système de lecture/écriture comme sur des fichiers classiques. Ils sont créés dynamiquement (traditionnellement par *udev*).
- **`/proc`** : système de fichiers virtuel **procfs** monté sur `/proc`, qui expose des informations sur les processus et le noyau ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=%60%2Fopt%60%20Add,the%20beginning%20of%20the%20boot)). Ce n’est pas un répertoire sur disque mais une interface fournie par le noyau. On y trouve un sous-dossier numérique par processus (ex: `/proc/1234/...` pour le PID 1234, contenant ses descripteurs de fichiers, mémoire, arguments…), ainsi que des fichiers spéciaux comme `/proc/cpuinfo` (infos CPU), `/proc/meminfo` (mémoire), `/proc/modules` (modules noyau chargés), etc. C’est un moyen important d’observer et interagir avec l’état du système via de simples lectures/écritures de fichiers.
- **`/sys`** : autre système de fichiers virtuel (sysfs) monté sur `/sys`, introduit avec le kernel 2.6, qui expose une vue arborescente du matériel et des drivers du noyau. Il organise les périphériques par bus, par classe, etc., et permet également de configurer certains paramètres du noyau via des fichiers (par exemple, `/sys/class/net/eth0/` contient des infos sur l’interface réseau eth0). 
- **`/run`** : répertoire plus récent (FHS 3.0) destiné aux données volatiles d’état runtime (ex: PID files, sockets UNIX, infos d’uptime) qui étaient historiquement éparpillées (comme `/var/run`). Souvent monté en tmpfs, nettoyé à chaque boot ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,116%2C%20init%20%2C%20%20118)).
- **`/mnt`** et **`/media`** : points de montage pour des systèmes de fichiers externes. `/mnt` est souvent utilisé pour monter manuellement et temporairement une partition/disque, tandis que `/media` contient des sous-répertoires où sont montés automatiquement les médias amovibles (clés USB, CD/DVD) par les environnements de bureau ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,providing%20%20110%20and%20kernel)).
- **`/opt`** : répertoire destiné aux packages additionnels (*optional*). On y installe parfois des logiciels tiers ou additionnels qui ne suivent pas forcément la logique de l’arborescence standard (par exemple `/opt/lampp` pour XAMPP, etc.) ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=%60%2Fopt%60%20Add,97%20for%20the%20root%20user)).
- **`/srv`** : contient des données spécifiques à des services fournis par la machine (*serveur*). Par exemple, on peut y mettre les fichiers d’un serveur web (`/srv/www/`), d’un dépôt CVS, etc., pour indiquer “service data” ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,fsck%20%2C%20%20117%2C%20route)).

Cette organisation standardisée (définie par FHS) permet aux administrateurs et utilisateurs de s’y retrouver facilement, et aux logiciels de savoir où placer ou chercher leurs fichiers. Ainsi, un programme pourra supposer que les configurations système sont dans `/etc`, que les logs vont dans `/var/log`, que les exécutables sont dans `/usr/bin` ou `/usr/local/bin` selon qu’ils sont distribués ou locaux, etc. La plupart des distributions Linux respectent scrupuleusement FHS, avec parfois quelques légères variantes ou liens symboliques (ex : certaines distributions unifient `/bin` et `/usr/bin` en faisant de l’un un lien vers l’autre pour simplifier l’arborescence ([Filesystem Hierarchy Standard - Wikipedia](https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard#:~:text=,22))). Mais globalement, un utilisateur passant d’une distribution à l’autre retrouvera la même structure de base, héritée de Unix.

## 6. Pilotes de périphériques 
Les **pilotes de périphériques** (device drivers) sont des composants du noyau chargés de la communication avec le matériel ou avec des périphériques virtuels. Leur rôle est de fournir une interface standard aux programmes pour utiliser un matériel donné, en se chargeant de tous les détails bas-niveau spécifiques à ce matériel. Sous Linux, quasiment tout est piloté par des drivers : les disques, les clés USB, les cartes réseau, les cartes graphiques, le son, mais aussi des éléments plus virtuels comme le système de fichiers pseudo `proc` ou les terminaux virtuels.

Du point de vue du noyau, un driver s’enregistre généralement comme un certain **type de périphérique** :
- **Pilotes de caractère** (**char drivers**) – ils gèrent des périphériques accessibles comme un flux de bytes, octet par octet ([c - Device driver classification - Stack Overflow](https://stackoverflow.com/questions/25756121/device-driver-classification#:~:text=1,of%20data%20at%20a%20time)). Typiquement, ce sont les ports série, les terminaux, les périphériques d’entrée (clavier) ou encore `/dev/tty`, `/dev/random`, etc. Les opérations se font via des appels système comme `read()`/`write()` qui transfèrent des octets en séquence.
- **Pilotes de bloc** – ils gèrent des périphériques organisés en blocs adressables (typiquement 512 octets ou plus) ([c - Device driver classification - Stack Overflow](https://stackoverflow.com/questions/25756121/device-driver-classification#:~:text=1,of%20data%20at%20a%20time)). Ce sont principalement les **disques** et autres supports de stockage (SSD, CD-ROM). Le noyau les utilise via le cache de blocs et les appels comme `read()/write()` en bloc, et ils permettent notamment de monter des systèmes de fichiers.
- **Pilotes réseau** – ils ne s’exposent pas comme des fichiers dans `/dev` mais dans la pile réseau du noyau. Leur interface est plus complexe : ils échangent des **trames** (frames) ou paquets réseau plutôt que de simples octets ou blocs ([c - Device driver classification - Stack Overflow](https://stackoverflow.com/questions/25756121/device-driver-classification#:~:text=2,file%20storage%20devices)). Exemples : driver d’interface Ethernet, Wi-Fi (émission/réception de paquets), interface loopback. L’accès se fait via l’API socket/Berkeley du côté user, qui s’appuie en interne sur ces drivers réseau.

En pratique, les drivers Linux peuvent être soit **intégrés statiquement** au noyau, soit compilés en **modules** chargeables (LKM) comme vu plus haut. La plupart des distributions choisissent de livrer la majorité des drivers sous forme de modules, afin qu’ils ne soient chargés que si le matériel correspondant est présent. 

Lorsque le système détecte un nouveau matériel (par exemple insertion d’une clé USB), un mécanisme comme **udev** va éventuellement charger le module kernel approprié pour le gérer. Inversement, un module de pilote inutilisé peut être déchargé pour libérer de la mémoire. On peut lister les drivers (modules) actuellement chargés avec la commande **`lsmod`**, qui affiche la liste des modules du noyau en mémoire. Par exemple, on y verra des entrées comme `usb_storage`, `i915` (driver GPU Intel), etc., avec leur taille et combien de fois ils sont utilisés. Techniquement, `lsmod` ne fait qu’afficher le contenu de `/proc/modules` de façon formatée ([Lsmod Command in Linux (List Kernel Modules) | Linuxize](https://linuxize.com/post/lsmod-command-in-linux/#:~:text=,in%20a%20nicely%20formatted%20list)). 

Pour gérer les modules, on utilise la commande **`modprobe`**. `modprobe` permet de **charger** un module (optionnellement avec des paramètres) ou de **retirer** (`modprobe -r`) un module du noyau. Elle est plus intelligente que l’ancien `insmod` : `modprobe` consulte les dépendances entre modules (décrites dans `/lib/modules/<version>/modules.dep`) et charge au préalable les autres modules dont dépend celui demandé. Par exemple, si on fait `modprobe snd_pcm`, cela pourra charger aussi `snd_timer` ou autres si nécessaire. De même, `modprobe -r` tentera de décharger non seulement le module cible mais aussi ceux qui ne seraient plus référencés. En général, udev appelle `modprobe` automatiquement lors de la détection de matériel. Le fichier `/etc/modules` ou les configurations dans `/etc/modprobe.d/` peuvent lister des modules à charger dès le démarrage. Les modules du noyau sont stockés dans le répertoire `/lib/modules/<version>/` et sont associés à la version précise du noyau (pas d’ABI stable : un module compilé pour un noyau X ne se charge pas sur un noyau Y différent sans recompilation) ([Linux kernel - Wikipedia](https://en.wikipedia.org/wiki/Linux_kernel#:~:text=Loadable%20kernel%20modules%20,part%20of%20the%20kernel%20executable)).

En résumé, les pilotes de périphériques sont le code du noyau qui fait le lien avec le matériel. Ils exposent souvent une abstraction (par ex. un fichier dans `/dev` ou une interface réseau) que les programmes peuvent utiliser via les appels système standard. Grâce aux drivers, les applications n’ont pas besoin de connaître les détails du fonctionnement d’une carte réseau ou d’un disque : elles utilisent des appels génériques (`open`, `ioctl`, `read`, `write`…), et c’est le driver qui, en coulisse, traduira ces requêtes en opérations concrètes sur le matériel. La qualité et la richesse de la logithèque de drivers font la force de Linux : aujourd’hui, le noyau supporte une quantité immense de matériels différents. Des commandes utiles pour interagir avec les drivers sont par exemple :
- `lsmod` – lister les modules chargés (donc les drivers actifs).
- `modinfo <module>` – afficher des informations sur un module (version, description, licence, alias de matériel pris en charge).
- `lspci`, `lsusb` – lister les périphériques PCI/USB connectés (pour savoir quels matériels sont présents et quels modules peuvent être associés).
- Fichiers dans `/proc` ou `/sys` – ex : `/proc/interrupts` pour voir quels drivers utilisent quelles IRQ, `/sys/bus/usb/devices/.../driver` pour voir quel driver gère un périphérique USB spécifique, etc.

## 7. Bibliothèques partagées et statiques 
Les **bibliothèques** sont des collections de fonctions/utilitaires partagées par plusieurs programmes. Sous Linux (et Unix en général), il existe deux modes principaux de liaison aux bibliothèques : la liaison **statique** et la liaison **dynamique** (*partagée*). 

- **Bibliothèques statiques** : ce sont des fichiers `.a` (archive) qui contiennent du code objet. Lors de la compilation/édition de liens d’un programme, on peut lier statiquement certaines libs – le code de la bibliothèque nécessaire est alors copié **dans l’exécutable** final. L’avantage est que l’exécutable n’a pas besoin de dépendance externe à l’exécution (il est auto-suffisant), mais l’inconvénient est une taille plus grande et une duplication du même code en mémoire si plusieurs programmes utilisent la même lib.
- **Bibliothèques dynamiques/partagées** : ce sont les fichiers `.so` (*shared object*). Ici, l’exécutable n’embarque pas le code de la lib ; il contient seulement une référence indiquant “j’aurai besoin de telle bibliothèque partagée”. À l’exécution, le système va **charger** la bibliothèque partagée en mémoire (une seule fois même si plusieurs processus en ont besoin) et la lier au programme. Ainsi, plusieurs applications peuvent **partager** une même bibliothèque en mémoire, ce qui réduit significativement l’empreinte globale. De plus, les mises à jour sont centralisées : corriger un bug dans la bibliothèque profite immédiatement à tous les programmes qui l’utilisent (il n’est pas nécessaire de recompiler chaque application) ([What is a real purpose of dynamic linking in c++? - Stack Overflow](https://stackoverflow.com/questions/77210770/what-is-a-real-purpose-of-dynamic-linking-in-c#:~:text=1,improvements%20once%20for%20all%20applications)). Ces raisons – **réduction de la taille sur disque**, **réduction de l’usage mémoire** et **mutualisation des mises à jour** – ont rendu la liaison dynamique largement majoritaire dans les systèmes modernes ([What is a real purpose of dynamic linking in c++? - Stack Overflow](https://stackoverflow.com/questions/77210770/what-is-a-real-purpose-of-dynamic-linking-in-c#:~:text=1,improvements%20once%20for%20all%20applications)).

Sous Linux, presque toutes les applications utilisent des bibliothèques partagées (à commencer par la **glibc**, la bibliothèque C standard, qui fournit les appels système, gestion de mémoire, fonctions standard, etc.). Lorsqu’un programme est lancé, c’est le **linker dynamique** (`ld-linux.so`, aussi appelé *runtime loader*) qui se charge de trouver et charger les `.so` requis en mémoire. Il consulte pour cela les chemins standards (`/lib`, `/usr/lib`…) et éventuellement la variable d’environnement `LD_LIBRARY_PATH` ou le cache `/etc/ld.so.cache` généré par `ldconfig`. Si une bibliothèque nécessaire est manquante, le programme ne pourra pas démarrer (erreur *“libXYZ.so.1 not found”*). 

Pour savoir de quelles bibliothèques partagées dépend un exécutable, on utilise la commande **`ldd`** (pour **l**i**d** Dynamic Dependencies). Par exemple, `ldd /bin/ls` listera toutes les libs nécessaires à `/bin/ls` (libc, libpthread, libselinux, etc.) et où elles sont trouvées dans le système ([ldd(1) - Linux manual page](https://man7.org/linux/man-pages/man1/ldd.1.html#:~:text=ldd%20prints%20the%20shared%20objects,and%20output%20is%20the%20following)) ([ldd(1) - Linux manual page](https://man7.org/linux/man-pages/man1/ldd.1.html#:~:text=%24%20ldd%20%2Fbin%2Fls%20linux,64.so.2%20%280x00005574bf12e000)). En interne, `ldd` fonctionne en exécutant le loader dynamique en mode spécial (avec `LD_TRACE_LOADED_OBJECTS`) afin d’afficher ces informations sans réellement lancer le programme ([ldd(1) - Linux manual page](https://man7.org/linux/man-pages/man1/ldd.1.html#:~:text=In%20the%20usual%20case%2C%20ldd,shared%20dependencies%20are%20special%3B%20see)). C’est un outil très utile pour diagnostiquer des problèmes de dépendances manquantes ou simplement pour comprendre quelles bibliothèques un binaire utilise.

Un autre outil précieux est **`strace`**, qui permet de tracer les appels système effectués par un programme. Bien que strace ne soit pas spécifique aux bibliothèques, on peut s’en servir pour voir le processus de chargement : en lançant une application avec `strace`, on verra notamment des appels `open("/lib/xyz.so", ...)` cherchant les bibliothèques, puis des `mmap()` qui les mappent en mémoire, etc. Strace montre *la fine couche entre un processus utilisateur et le noyau* ([Understanding system calls on Linux with strace | Opensource.com](https://opensource.com/article/19/10/strace#:~:text=A%20system%20call%20is%20a,processes%20and%20the%20Linux%20kernel)), ce qui inclut l’ouverture des libs, la recherche de fichiers de config (par ex. `/etc/ld.so.cache`), etc. En cas d’erreur de chargement, strace peut révéler par exemple que le processus cherche une lib dans tel répertoire et obtient `ENOENT` (fichier non trouvé), aidant ainsi à corriger le problème (installer la lib manquante, ajuster `LD_LIBRARY_PATH`, etc.). 

En résumé, les bibliothèques partagées (.so) sont un pilier des systèmes Linux, permettant un **partage de code** efficace entre applications. Des outils comme `ldd` permettent d’auditer les dépendances d’un programme, et `strace` d’observer dynamiquement le fonctionnement d’un processus (y compris le chargement de ces dépendances). Il est généralement recommandé de lier dynamiquement sauf cas d’usage spécifiques (logiciel embarqué ultra-portable, ou exigence d’avoir un binaire unique ne dépendant de rien). À l’inverse, la liaison statique peut être utile pour des environnements minimalistes ou pour s’affranchir de bibliothèques système (au prix d’une duplication du code). Linux supporte les deux, mais la plupart des distributions n’utilisent que très rarement les .a (parfois même ne fournissent pas les .a par défaut, uniquement les .so et les headers de dev, afin de pousser à la dynamique).

## 8. Appels systèmes 
Les **appels système** (*system calls* ou *syscalls*) forment l’interface de communication entre les programmes en espace utilisateur et le noyau. Lorsqu’un programme a besoin d’un service que seul le noyau peut rendre (par exemple lire un fichier, allouer de la mémoire, créer un processus), il exécute un appel système. Un syscall s’apparente à un appel de fonction, à la différence qu’il entraîne un passage du **mode utilisateur au mode noyau** via un mécanisme de trap/interrupt matériel ([Understanding system calls on Linux with strace | Opensource.com](https://opensource.com/article/19/10/strace#:~:text=System%20calls%20are%20very%20similar,using%20a%20special%20trap%20mechanism)). En quelque sorte, c’est une “porte” d’entrée contrôlée vers le noyau. Le programme utilisateur exécute une instruction spéciale (ex: `syscall` sur x86_64, ou `int 0x80` sur x86, ou un SVC sur ARM) qui provoque une interruption logicielle et transfère le contrôle au noyau (anneau 0) à l’adresse du gestionnaire d’appel système. Le noyau va alors effectuer l’action demandée (si elle est autorisée) puis revenir en espace utilisateur avec le résultat.

Linux offre une panoplie d’appels système (plus de 300). On y retrouve des appels pour la gestion des processus (par ex. `fork()` qui crée un processus en dupliquant le processus courant, ou `execve()` qui charge un nouvel exécutable dans le processus courant), pour la gestion de la mémoire (`mmap()`, `brk()`), des fichiers (`open()`, `read()`, `write()`, `close()` pour ouvrir et manipuler des descripteurs de fichier, `stat()` pour obtenir les métadonnées d’un fichier), pour le réseau (`socket()`, `send()`, `recv()`…), etc. Par exemple, `open()` permet à un processus de demander au noyau d’ouvrir un fichier sur le disque et de recevoir un descripteur de fichier en retour. `fork()` demande au noyau de créer un duplicata du processus courant – le noyau va copier les structures nécessaires et renvoyer 0 dans le processus fils nouvellement créé (et l’PID du fils dans le parent). `execve()` demande au noyau de charger un autre programme (binaire ELF) à la place du code courant – le noyau va lire le fichier binaire, l’associer au processus existant et lancer son exécution. Ces appels forment l’API fondamentale du système d’exploitation. Linux implémente environ **380 appels système** distincts sur x86_64 (le nombre exact évolue avec les versions du kernel) ([User space and kernel space - Wikipedia](https://en.wikipedia.org/wiki/User_space_and_kernel_space#:~:text=aim%20to%20be%20POSIX%20%2F,145%20subsystem%20%20Networking%20subsystem)), couvrant toutes les fonctionnalités POSIX et plus. La liste des syscalls Linux inclut par exemple `clone()` (variantes avancées de fork, utilisées pour les threads ou containers), `ioctl()` (appel générique pour envoyer des commandes spécifiques à un périphérique), `select()`/`poll()`/`epoll()` (multiplexage d’E/S), `kill()` (envoi de signal), etc.

Pour l’utilisateur ou le programmeur, ces appels système sont exposés via des fonctions de la glibc (par exemple la fonction de bibliothèque `fork()` encapsule en réalité le syscall `clone` avec les bons paramètres, de même `printf` finira par appeler `write()` etc.). On ne voit donc pas explicitement “syscall 64” dans son code, on appelle simplement la fonction C correspondante, mais celle-ci effectue bien un trap vers le noyau. Chaque appel système est identifié par un numéro (par architecture) et possède un stub dans la glibc pour le convoquer.

Du point de vue du suivi et du débogage, **`strace`** est l’outil par excellence pour observer les appels système. En lançant un programme avec `strace`, celui-ci va “espionner” toutes les interactions système : on verra en temps réel chaque appel système effectué, avec ses arguments et le code de retour. Par exemple, un `ls` tracé avec strace montrera les `open("/etc/ld.so.cache")`, `open("/lib/libc.so.6")` (chargement des libs), puis des appels `open()` des répertoires, des `getdents()` pour lire les entrées de répertoire, etc., jusqu’à l’appel `exit()` final. Strace permet de **visualiser la frontière entre l’espace utilisateur et le noyau** ([Understanding system calls on Linux with strace | Opensource.com](https://opensource.com/article/19/10/strace#:~:text=A%20system%20call%20is%20a,processes%20and%20the%20Linux%20kernel)), ce qui est extrêmement utile pour comprendre le comportement d’un programme ou diagnostiquer une erreur (fichier non trouvé, permission refusée, etc., visibles via les codes d’erreur des syscalls). Par exemple, si un programme plante, strace montrera le dernier syscall appelé et l’erreur éventuellement reçue. 

En résumé, les appels système constituent le **contrat d’interface** entre le noyau Linux et les applications. Ils fournissent un ensemble de services (gestion des processus, mémoire, fichiers, communication inter-processus, réseau, etc.) qu’un programme peut invoquer. Cette interface est relativement stable et standardisée (POSIX), ce qui permet à des logiciels d’être portables : recompiler un programme POSIX sur Linux utilisera les appels système Linux correspondants via la glibc. La performance des appels système est critique, c’est pourquoi le noyau et les CPU offrent des optimisations (par ex., sur x86_64 l’instruction `syscall` est bien plus rapide que l’ancienne interruption logicielle). Linux fournit également des mécanismes comme le *vdso* (Virtual Dynamic Shared Object) qui permet d’exposer certaines routines noyau en espace utilisateur sans trap (par ex, gettimeofday peut se faire sans appel système en lisant une zone mémoire partagée, pour éviter le coût du trap). Néanmoins, pour la plupart des opérations, le passage en mode noyau est indispensable. Strace, en tant qu’outil d’observation, nous rappelle à quel point chaque action d’un programme utilisateur repose sur ces appels au noyau – c’est le garde-fou qui sépare le monde utilisateur (restreint) des ressources réelles du système.

## 9. Différents *Shells* (Bash, Zsh, Fish, etc.) 
Le *shell* est l’interpréteur de commandes dans un système Unix/Linux. Il en existe plusieurs, avec des fonctionnalités et philosophies légèrement différentes, tout en remplissant le même rôle de base : permettre à l’utilisateur d’exécuter des commandes, d’écrire des scripts, d’automatiser des tâches. Sous Linux, le shell par défaut est souvent **Bash** (Bourne Again Shell), mais d’autres shells populaires incluent **Zsh** (Z Shell), **Fish** (Friendly Interactive Shell), **Tcsh**, **Ksh**, etc. Comparons Bash, Zsh et Fish qui sont fréquemment utilisés :

- **Bash** : C’est le shell par défaut sur la plupart des distributions Linux (et également disponible sur macOS, BSD, etc.). Bash est un descendant du shell Bourne (`sh`) avec de nombreuses améliorations. Il est apprécié pour sa **stabilité** et sa large compatibilité POSIX (on peut écrire des scripts Bash qui fonctionneront quasiment partout). Bash offre toutes les fonctions classiques : scripting, variables, redirections, historique, complétion baseline, etc. Il est toutefois un peu spartiate en mode interactif par défaut (complétion basique par Tab, pas de coloration syntaxique native). Son avantage est d’être omniprésent et léger. Pour les scripts shell, c’est généralement Bash qui est utilisé, car il est disponible partout et conforme POSIX.

- **Zsh** : Z Shell est un shell puissant et très **personnalisable**. Il intègre par défaut de nombreuses fonctionnalités avancées absentes de Bash ou qui nécessitaient des bricolages : complétion programmable très poussée, **glob** (sélection de fichiers) extrêmement flexible, support des **plugins/thèmes** via des frameworks comme *Oh My Zsh*, chargement dynamique de modules, autocorrection des fautes de frappe, etc. Zsh est souvent choisi par les utilisateurs avancés pour une expérience interactive plus riche. Par exemple, grâce à Oh My Zsh ou Prezto, on peut avoir un prompt informatif (git, etc.), une complétion contextuelle exhaustive, des abréviations, etc. Zsh est également un très bon shell de scripting, proche de Bash dans sa syntaxe (il est largement compatible, avec quelques différences subtiles). En somme, Zsh fournit une **expérience améliorée** par rapport à Bash, au prix d’un shell un peu plus lourd et complexe. On le retrouve par défaut sur macOS depuis quelques années. On peut résumer ainsi : *Bash offre la stabilité et la compatibilité universelle, Zsh apporte des fonctionnalités avancées et une grande customisation* ([Bash, Zsh, or Fish: Which Shell Environment Is Right for You?](https://medium.com/devquicktips/bash-zsh-or-fish-which-shell-environment-is-right-for-you-52d46eb7b2dd#:~:text=Bash%2C%20Zsh%2C%20or%20Fish%3A%20Which,friendliness%20and%20simplicity)).

- **Fish** : Fish se distingue par son approche *user-friendly* et *modernité*. Contrairement à Bash/Zsh qui héritent de la syntaxe historique sh, Fish a été conçu pour être plus simple et sans configuration initiale. Par défaut, Fish propose une **autocomplétion contextuelle** intelligente (affiche les possibilités de commandes/arguments dès la saisie, sans appuyer Tab), une **coloration syntaxique** immédiate (commandes reconnues en surbrillance, erreurs de syntaxe soulignées avant même d’exécuter), et intègre des fonctionnalités comme les suggestions automatiques basées sur l’historique (affichées en gris pendant qu’on tape). Fish n’utilise pas le concept d’alias comme Bash, mais encourage l’utilisation de **fonctions** universelles à la place. Sa syntaxe diffère légèrement de Bash (par exemple pour les variables, on n’écrit pas `$VAR` dans les scripts fish, mais `$VAR` en ligne de commande reste valable ; les structures de contrôle diffèrent aussi). L’objectif de Fish est qu’un utilisateur ait une expérience riche *sans avoir à installer plugins ou modifier des configs* – tout fonctionne *out of the box*. C’est pourquoi beaucoup apprécient Fish pour un usage interactif quotidien, en particulier les débutants ou ceux qui ne veulent pas passer du temps à tuner leur shell. En contrepartie, Fish est **moins adapté au scripting classique** car non POSIX : un script fish ne fonctionnera qu’avec fish. Il est donc rarement utilisé pour des scripts système (on lui préfère Bash dans ces cas). Fish excelle vraiment comme shell interactif, offrant simplicité d’utilisation et agrément visuel. On peut dire : *Bash offre la compatibilité et la sobriété, Zsh l’extensibilité pour les power-users, Fish la convivialité prête à l’emploi* ([Bash, Zsh, or Fish: Which Shell Environment Is Right for You?](https://medium.com/devquicktips/bash-zsh-or-fish-which-shell-environment-is-right-for-you-52d46eb7b2dd#:~:text=Bash%2C%20Zsh%2C%20or%20Fish%3A%20Which,friendliness%20and%20simplicity)).

Chacun de ces shells a ses adeptes et ses cas d’usage : 
- **Bash** reste le standard de facto pour l’écriture de scripts et l’administration système, grâce à sa disponibilité universelle et sa conformité POSIX. Même un utilisateur de Zsh ou Fish utilisera souvent Bash en arrière-plan pour exécuter des scripts `/bin/sh`.
- **Zsh** est souvent choisi par les utilisateurs cherchant à optimiser leur environnement de travail avec des prompts améliorés, des complétions intelligentes et un haut degré de personnalisation. Avec les bons réglages, Zsh peut considérablement accélérer la saisie de commandes complexes.
- **Fish** convient bien à ceux qui veulent un shell interactif efficace sans effort de configuration, ou aux débutants pour qui les aides visuelles de Fish rendent l’utilisation du terminal plus accessible. 

Il est à noter que tous ces shells remplissent au fond le même rôle et permettent d’accomplir les mêmes tâches ; passer de l’un à l’autre est surtout affaire de confort et d’habitude. Un script Bash classique pourra généralement être exécuté sous Zsh (qui est largement compatible) mais pas tel quel sous Fish (non compatible par défaut). Certains utilisateurs avancés utilisent même plusieurs shells selon les contextes (par ex, Bash pour les scripts, Zsh pour l’interactif sur leur PC, Fish sur une autre machine pour tester). Grâce à l’abstraction du shell, l’environnement utilisateur peut être modulé sans toucher aux couches inférieures du système.

## 10. La virtualisation sous Linux 
 ([ Containers vs Virtual Machines | Atlassian ](https://www.atlassian.com/microservices/cloud-computing/containers-vs-vms)) *Comparaison entre l’architecture à base de **machines virtuelles** (gauche) et celle à base de **conteneurs** (droite). Dans un scénario virtualisation traditionnelle, un hyperviseur (ou un noyau hôte dans le cas de KVM) permet d’exécuter plusieurs OS invités complets (chaque VM ayant son propre kernel et ses bibliothèques). À droite, avec la conteneurisation, un seul noyau hôte est partagé, et un *moteur de conteneurs* isole plusieurs applications dans des environnements séparés mais plus légers (pas de duplicata du système d’exploitation complet).* 

Linux offre plusieurs approches de **virtualisation**. On peut distinguer la virtualisation de type *machine virtuelle* (hyperviseur simulant un hardware complet pour chaque invité) et la virtualisation de type *conteneur* (isolation au niveau du système d’exploitation). 

– **Virtualisation complète avec hyperviseur (KVM, Xen, etc.)** : Linux peut servir aussi bien d’**hyperviseur de type 1** que de type 2. Le projet le plus emblématique est **KVM (Kernel-based Virtual Machine)**, intégré directement au noyau Linux. KVM transforme le noyau en hyperviseur : il utilise un module noyau (`kvm.ko`) qui exploite les extensions de virtualisation matérielle des CPU (Intel VT-x, AMD-V) pour créer des machines virtuelles. Chaque VM KVM est en fait un processus Linux (géré par le planificateur standard) associé à un espace d’adressage séparé pour l’OS invité ([Xen vs. KVM: A comparison - IONOS](https://www.ionos.com/digitalguide/server/know-how/xen-vs-kvm/#:~:text=KVM%20vs,performance%20compares)). Le noyau Linux hôte joue alors le rôle d’hyperviseur en arbitrant l’accès au CPU, à la mémoire et aux périphériques entre les différentes VMs. L’avantage de KVM est qu’il bénéficie de tout l’écosystème Linux (ordonnanceur efficace, pilotes existants pour gérer les I/O virtuelles via virtio, etc.). En somme, KVM est vu comme un hyperviseur **intégré au kernel** (certains le qualifiaient de *type 2* car il s’appuie sur un OS hôte, mais étant dans le noyau, il s’apparente à du type 1 dans les faits). 

À l’inverse, **Xen** est un hyperviseur historique de type 1 qui fonctionne en surcouche du matériel, indépendamment d’un OS hôte. Au démarrage, le hyperviseur Xen se charge en premier (à la place d’un noyau classique) et crée un premier environnement privilégié (le *dom0*) qui exécute Linux pour piloter le matériel. Les autres VMs (domU) s’exécutent soit via **paravirtualisation** (l’OS invité sait qu’il est virtualisé et appelle Xen pour certaines opérations, ce qui nécessite un OS modifié, mais offre de hautes performances ([Xen vs. KVM: A comparison - IONOS](https://www.ionos.com/digitalguide/server/know-how/xen-vs-kvm/#:~:text=How%20Xen%20uses%20paravirtualization%20to,boost%20efficiency))), soit via **full virtualisation** avec extensions CPU (Xen émule alors un matériel standard si l’OS n’est pas modifié, au prix d’un peu plus de overhead). Xen offre un haut degré d’isolement et a longtemps été prisé dans les environnements serveurs, bien qu’il soit plus complexe à mettre en place (puisqu’il implique un hyperviseur dédié). Aujourd’hui, KVM a largement gagné en popularité car il est directement inclus dans Linux (pas besoin d’un hyperviseur externe), bénéficiant du développement communautaire et d’un entretien plus aisé (pas de patchs externes au kernel à maintenir) ([Why is the market moving away from xen to kvm? : r/sysadmin - Reddit](https://www.reddit.com/r/sysadmin/comments/7cjpe8/why_is_the_market_moving_away_from_xen_to_kvm/#:~:text=Reddit%20www,addition%2C%20a%20KVM%20VM)) ([Xen vs. KVM, the hypervisors compared - IONOS](https://www.ionos.com/digitalguide/server/know-how/xen-vs-kvm/#:~:text=Xen%20vs,into%20the%20Linux%20kernel)). En pratique, des solutions de virtualisation comme **QEMU/KVM** (libvirt, etc.) utilisent KVM pour la performance et QEMU pour émuler les périphériques, fournissant une expérience équivalente à celle de VirtualBox, VMWare, etc., entièrement sur Linux. 

– **Conteneurs (LXC, Docker, etc.)** : Plutôt que d’émuler une machine complète, Linux propose l’**isolement par conteneur**, souvent appelé virtualisation au niveau OS. Cette approche utilise les fonctionnalités du noyau telles que les **namespaces** (espaces de noms) et les **cgroups** (groupes de contrôle des ressources) pour isoler des processus les uns des autres comme s’ils tournaient sur des systèmes différents, alors qu’ils partagent le même noyau. Des projets comme **LXC (Linux Containers)** ont été les pionniers, et plus récemment l’écosystème **Docker** a popularisé cette approche en facilitant le déploiement d’applications dans des conteneurs légers. 

Dans un conteneur, on va isoler les vues des **processus**, du **système de fichiers**, du **réseau**, des **utilisateurs**, etc., de sorte qu’un processus dans le conteneur pense être seul sur la machine. Cependant, tous les conteneurs partagent le noyau **Linux hôte** : ils n’embarquent pas de kernel propre. Ainsi, lancer 10 conteneurs n’instancie pas 10 noyaux, juste 10 groupes de processus isolés. Cela apporte une énorme **légèreté** : on peut démarrer un conteneur en quelques millisecondes, consommer très peu de RAM supplémentaire (juste les processus en plus), et on évite la redondance des OS invités. L’impact sur les ressources est minime tant que les conteneurs ne surchargent pas le CPU/IO, et on peut en packer beaucoup sur une machine. L’**impact sur la gestion des ressources** est aussi très flexible : via les cgroups, on peut limiter finement l’usage CPU/RAM/IO de chaque conteneur pour assurer une qualité de service. Google a par exemple isolé ses workloads dans des conteneurs bien avant Docker, afin d’optimiser l’utilisation de ses serveurs. 

La contrepartie des conteneurs est qu’ils **partagent le noyau** : cela signifie qu’on ne peut pas faire tourner un OS non Linux dans un conteneur Linux (pas de Windows conteneur sur un Linux par ex., contrairement à une VM où on peut installer n’importe quel OS invité). De plus, la sécurité repose sur l’isolement du noyau ; si un attaquant brise l’isolement (via une vulnérabilité kernel), il peut sortir du conteneur. Néanmoins, en pratique, les conteneurs Linux offrent un isolement suffisant pour de nombreux usages et permettent une densité très supérieure aux VMs. C’est un peu un retour aux *chroot*/*jails* historiques, en bien plus abouti : chaque conteneur a sa propre vue du système (montages privés, réseau virtuel, utilisateurs isolés) tout en étant beaucoup plus **agile** qu’une VM. Docker a ajouté à cela un format d’images empilables et un écosystème qui a révolutionné le déploiement logiciel. 

**Gestion des ressources** : Dans une solution de virtualisation classique (KVM/Xen), on réserve souvent à chaque VM une quantité de RAM fixe, un nombre de vCPU, etc. Le scheduler du noyau hôte (ou l’hyperviseur) se charge de répartir le temps CPU entre les VMs, éventuellement en pondérant par priorité. La mémoire, une fois allouée à une VM, lui est dédiée (sauf techniques de sur-commit ou de ballon mémoire). Dans le cas des conteneurs, la gestion est plus souple car tous les processus partagent le même scheduler et le même pool de mémoire : on peut configurer via cgroups des limites ou des priorités, mais c’est plus dynamique (si un conteneur n’utilise pas sa RAM allouée, un autre peut en profiter librement, il n’y a pas de cloisonnement strict à moins de le demander). Cela permet souvent une utilisation plus efficace des ressources globales. Cependant, cela implique aussi que des conteneurs très actifs peuvent influencer l’hôte (ex: charger le kernel par de nombreuses requêtes). Des mécanismes comme les *quotas CPU* ou *limitations de mémoire* via cgroups atténuent ce problème en limitant ce qu’un conteneur peut consommer.

En pratique, les environnements de **cloud** et d’orchestration combinent souvent ces approches : par ex. un orchestrateur Kubernetes déploiera des conteneurs Docker sur des VM. Linux est au cœur de ces deux technologies : en tant qu’hyperviseur KVM pour faire tourner des VM dans les cloud publics, et en tant que moteur de conteneurs (avec Docker/Containerd) pour isoler les applications. Ainsi, la virtualisation sous Linux offre un **spectre complet** : de la VM lourde mais totalement flexible (n’importe quel OS invité) au conteneur léger mais même noyau, en passant par des compromis intermédiaires (LXC permet par ex. d’avoir un environnement proche d’une VM mais sans hyperviseur). Cette polyvalence fait de Linux l’épine dorsale de la plupart des infrastructures virtualisées modernes. Linux fournit dans sa documentation et son code des sous-systèmes complets pour la virtualisation (ex: KVM pour l’hyperviseur, cgroups/namespace pour les conteneurs, KVM API via `/dev/kvm`, interfaces /proc pour cgroups, etc.), et continue d’innover (ex : projets comme **Kata Containers** qui combinent VM légère et isolation de conteneur, ou **virtio-fs** pour partager un système de fichiers efficacemment entre hôte et VM, etc.). En conclusion, que ce soit pour exécuter plusieurs systèmes complets sur une seule machine ou isoler finement des applications, Linux propose des solutions de virtualisation robustes et performantes, avec un impact maîtrisé sur la gestion des ressources grâce aux optimisations du noyau et du matériel ([ Containers vs Virtual Machines | Atlassian ](https://www.atlassian.com/microservices/cloud-computing/containers-vs-vms#:~:text=technologies,above%20the%20operating%20system%20level)). 

