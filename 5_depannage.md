Je vais préparer une explication détaillée sur le dépannage matériel sous Debian et d'autres distributions Linux, avec une approche spécifique aux serveurs. Je vais inclure des outils spécifiques à Debian ainsi que des outils génériques compatibles avec plusieurs distributions Linux. Je fournirai également de nombreux exemples concrets avec des commandes pratiques et leur interprétation.

### 1. Types de problèmes matériels

Les serveurs peuvent rencontrer divers problèmes matériels courants affectant le **CPU**, la **mémoire RAM**, le **stockage**, l’**alimentation** ou le **réseau**. Par exemple, une surchauffe ou un défaut du CPU peut provoquer des ralentissements, des **kernel panic** ou des arrêts subits du système. Une panne de **RAM** (barrette défectueuse) peut causer des erreurs aléatoires, comme des processus qui se terminent de façon inattendue ou des **segmentation faults**, voire des crashs du noyau qui semblent logiciels mais résultent en fait d’une défaillance matérielle ([How to troubleshoot hardware problems in Linux](https://www.dedoimedo.com/computers/linux-hardware-troubleshooting.html#:~:text=Some%20kind%20of%20errors%20may,gaming%C2%A0desktop%20case%20ground%20wiring%20issues)). Un disque de **stockage** (HDD/SSD) en fin de vie peut générer des erreurs d’**E/S** (entrées/sorties) dans les journaux, des secteurs illisibles ou des performances très dégradées. Une alimentation électrique défaillante peut entraîner des redémarrages intempestifs ou l’impossibilité de démarrer le serveur. Une carte **réseau** défectueuse peut provoquer des coupures réseau, une perte de connectivité ou des paquets erronés.

**Symptômes –** Les problèmes matériels se manifestent souvent par des symptômes tels qu’une **baisse de performances** inexpliquée, des erreurs récurrentes dans les journaux système (par ex. erreurs d’E/S sur un disque, messages de perte de lien réseau, corrections ECC mémoire), des **freeze** ou redémarrages aléatoires, ou encore des composants qui ne sont plus détectés par le système. Par exemple, des messages “I/O error” dans `dmesg` indiquent souvent un disque qui ne répond plus correctement, et de **faibles débits réseau** ou des pertes de paquets peuvent signaler un problème de câble ou de carte réseau. En présence de ce type de symptômes, il est crucial de déterminer si la cause est matérielle ou logicielle.

**Matériel vs. logiciel –** Différencier un problème matériel d’un problème logiciel n’est pas toujours évident. En général, un **problème matériel** se produit indépendamment de la configuration logicielle : il peut entraîner des comportements erratiques impossibles à reproduire à la demande, des corruptions de données ou des pannes même après une réinstallation du système. Par exemple, quelques cellules défectueuses dans de la RAM peuvent provoquer des plantages aléatoires d’applications ou du noyau, que l’on attribuerait à tort au logiciel alors que la cause est matérielle ([How to troubleshoot hardware problems in Linux](https://www.dedoimedo.com/computers/linux-hardware-troubleshooting.html#:~:text=Some%20kind%20of%20errors%20may,gaming%C2%A0desktop%20case%20ground%20wiring%20issues)). De même, des erreurs **MCE** (Machine Check Exception) rapportées dans `dmesg` suggèrent souvent un défaut matériel (CPU, RAM, carte mère). En revanche, un **problème logiciel** pur provoque généralement des erreurs cohérentes et reproductibles (un bug d’une application, un service qui plante dans un contexte précis) et n’entraîne pas de messages d’erreur liés au matériel dans les journaux du kernel. En cas de doute, on peut procéder par élimination : tester la machine avec un autre OS ou un *live CD*, vérifier l’absence d’erreurs dans les journaux matériels, ou utiliser des utilitaires de diagnostic matériel comme décrits ci-dessous.

### 2. Analyse du matériel

Pour dépanner efficacement un serveur Linux, il faut adopter une démarche méthodique d’**analyse du matériel** en utilisant les journaux système et des outils de diagnostic. Debian et Ubuntu fournissent la plupart de ces outils via leurs dépôts APT, tandis que sur RHEL/CentOS on les obtient avec *yum/dnf* et sur Arch via *pacman*. Nous comparerons les différences entre distributions lorsque pertinentes.

**Vérification des journaux système :** Les journaux sont le point de départ du diagnostic. La commande `dmesg` permet d’afficher les messages du noyau Linux en mémoire. On y trouve souvent des indices sur les pannes matérielles : par exemple des erreurs disque (I/O errors), des alertes thermiques CPU, des échecs de périphériques, etc. On peut filtrer ces messages : `dmesg | grep -i error` recherchera les occurrences du mot *error* (erreur) sans distinction de casse. De plus, sur les systèmes modernes avec systemd, `journalctl` regroupe tous les logs. L’option `-xe` affiche les derniers événements avec des détails additionnels et les messages d’erreur critiques. Par exemple, `journalctl -xe` après un incident peut révéler qu’un service s’est arrêté à cause d’une erreur matérielle (échec d’écriture disque, OOM Killer ayant tué un processus faute de RAM, etc.). Notez que l’emplacement des logs texte varie selon les distributions : **Debian/Ubuntu** enregistrent le journal global dans **`/var/log/syslog`**, alors que les systèmes **Red Hat (RHEL, CentOS)** utilisent **`/var/log/messages`** pour les messages système généraux ([Linux Logging Basics - The Ultimate Guide To Logging](https://www.loggly.com/ultimate-guide/linux-logging-basics/#:~:text=,and%20output%20from%20pluggable%20authentication)). Sur **Arch Linux**, il n’y a pas de fichier syslog/messages par défaut, il faut utiliser `journalctl` pour consulter le journal du système (ou installer un démon syslog si souhaité). Sur toute distribution, on peut suivre en temps réel les nouveaux logs avec `tail -f` (exemple : `tail -f /var/log/syslog` sous Debian, ou `tail -f /var/log/messages` sous RHEL) pour voir apparaître immédiatement les messages lors d’un événement matériel ([Troubleshooting hardware problems in Linux](https://www.redhat.com/en/blog/troubleshooting-hardware-problems-linux#:~:text=You%20can%20also%20look%20at,f%20%2Fvar%2Flog%2Fmessages%20command)).

**Surveillance des températures et de l’état matériel :** La surchauffe ou des tensions anormales peuvent causer des pannes, il est donc important de surveiller les **capteurs hardware**. L’outil standard est **lm-sensors**, un ensemble de pilotes et d’utilitaires pour lire les capteurs de température, de tension et de vitesse des ventilateurs ([lm_sensors - ArchWiki](https://wiki.archlinux.org/title/Lm_sensors#:~:text=lm_sensors%20,install%2C%20configure%2C%20and%20use%20lm_sensors)). Sur Debian/Ubuntu, installez-le via `apt install lm-sensors` (sur RHEL : `yum install lm_sensors`, sur Arch : `pacman -S lm_sensors`). Après installation, exécutez `sensors-detect` en root pour détecter les capteurs et charger les modules noyau adéquats. Ensuite, la commande `sensors` affiche les températures (CPU, cartes mères, etc.), par ex. `sensors` peut retourner des valeurs comme la température du CPU (par exemple *CPU: +45.0°C*). Pour les **disques**, l’utilitaire **hddtemp** peut donner la température d’un disque dur SATA (`hddtemp /dev/sda`), mais il devient obsolète sur certains systèmes modernes. En alternative, on peut utiliser `smartctl -A /dev/sda` (voir section stockage) qui fournit aussi la température SMART d’un disque, ou encore consulter les fichiers dans `/sys/class/thermal/` ou `/sys/class/hwmon/` sur les distributions récentes qui intègrent ces infos au sein du noyau. Une machine serveur dispose souvent de nombreux capteurs (température des CPU, des modules de RAM, température et vitesse des ventilateurs, etc.), leur surveillance proactive permet de prévenir les incidents (on peut par exemple programmer des alertes si une température dépasse un seuil). Sur les serveurs physiques, on utilisera parfois l’IPMI (`ipmitool sensor`) pour lire les capteurs embarqués via le BMC/DRAC, mais cela sort du cadre purement OS Linux. 

**Détection des pannes mémoire :** La mémoire RAM défectueuse est une cause fréquente de crash. Pour tester la RAM d’un serveur, on dispose de deux approches : en **production** ou en **hors-ligne**. En production (sans redémarrer), on peut utiliser **memtester** (via `apt install memtester` sur Debian/Ubuntu, ou `yum install memtester` sur RHEL EPEL) qui va allouer une partie de la RAM et y effectuer des séquences d’écriture/lecture pour détecter des erreurs. Par exemple, `memtester 512M 5` testera 512 Mo de RAM sur 5 passes et signalera toute erreur mémoire (une erreur indique quasiment à coup sûr une barrette défectueuse). Cependant, memtester ne peut pas tester la totalité de la RAM utilisée par le système. Pour une vérification exhaustive, on utilise **Memtest86+** en démarrant le serveur dessus (souvent proposé dans le menu GRUB des distributions après installation du paquet *memtest86+*). **Memtest86+** réalise une batterie de tests bas niveau sur toute la mémoire et identifie les cellules défectueuses. Les erreurs mémoire sont parmi les causes les plus communes de plantages et instabilités du système ([Memtest86+ | The Open-Source Memory Testing Tool](https://www.memtest.org/#:~:text=Troubleshoot%20PC%20Instabilities%20Memory%20errors,RAM%20is%20faulty%20or%20not)) – si Memtest86+ rapporte des erreurs (lignes rouges dans l’interface avec des adresses), il faut remplacer la barrette de RAM correspondante. En environnement serveur critique, on privilégie de la RAM **ECC** qui corrige automatiquement les erreurs simples et journalise les erreurs multiples dans les logs du BIOS/Linux (par ex. via `dmesg` on peut voir des messages ECC memory error corrected). Sur Linux, la commande `dmidecode -t memory` permet d’afficher des informations sur les modules RAM installés (capacités, emplacements DIMM, présence ou non d’ECC, etc.), ce qui aide à identifier physiquement les barrettes à tester ou remplacer.

**Vérification des performances CPU :** Pour diagnostiquer un problème lié au CPU, il faut à la fois inspecter les **caractéristiques** du processeur et sa **charge**. La commande `lscpu` fournit un résumé de l’architecture CPU : nombre de sockets, nombre de cœurs par socket, nombre de threads par cœur, modèle du processeur, fréquences, fonctionnalités (virtualisation, jeux d’instructions pris en charge, etc.) ([System insights with command-line tools: lscpu and lsusb - Fedora Magazine](https://fedoramagazine.org/system-insights-with-command-line-tools-lscpu-and-lsusb/#:~:text=lscpu%20%E2%80%93%20Display%20CPU%20information)). Par exemple, `lscpu` pourra indiquer *CPU(s): 16* (sur un bi-processeur 8 cœurs), *Thread(s) per core: 2* si l’hyperthreading est activé, etc. C’est utile pour vérifier que le système voit bien toute la CPU (par ex., si un cœur est manquant, cela peut indiquer un problème de configuration BIOS ou de CPU partiellement défectueuse). Ensuite, pour la charge CPU en temps réel, on utilise des outils de **monitoring** : `top` (inclus par défaut) ou son équivalent amélioré `htop` (à installer via paquet *htop*). La commande `top` affiche en continu la liste des processus et leur utilisation CPU, RAM, etc., ainsi que des moyennes de charge. `htop` offre une interface plus lisible avec utilisation par cœur, tri interactif, etc. On peut repérer si le CPU est saturé (par exemple 100% d’utilisation sur tous les cœurs, load average très élevé) ou si un processus monopolise le CPU. En complément, la suite **sysstat** (à installer via *sysstat*) offre `mpstat` qui permet d’afficher les statistiques d’utilisation CPU par cœur. Par exemple `mpstat -P ALL 1 5` affichera, à intervalle d’1 seconde, l’utilisation de chaque CPU (user, system, iowait, idle, etc.) sur 5 relevés. Une distribution comme RHEL installe souvent *sysstat* par défaut, sur Debian il faut l’ajouter. Un déséquilibre (par ex. un cœur toujours à 100% pendant que les autres sont idle) peut orienter le diagnostic (thread bloqué, interruption matérielle excessive sur un CPU, etc.). Si la performance CPU est moindre qu’attendu, on vérifiera aussi la **fréquence** de celui-ci : des CPU peuvent être bridés en fréquence en cas de surchauffe (throttling) ou par des réglages d’économie d’énergie. La commande `cpupower frequency-info` (paquet *cpupower* sur Debian/RedHat) donne l’état des fréquences. Enfin, pour des tests poussés, on peut utiliser des outils de stress test CPU (ex: `stress-ng`) afin de vérifier la stabilité du système sous charge maximale – utile pour révéler un problème de refroidissement ou d’alimentation.

**Surveillance de l’utilisation du réseau :** Les problèmes réseau matériels se diagnostiquent en examinant à la fois la configuration logicielle et l’état des interfaces. Pour lister les **interfaces réseau** actives et leurs paramètres, on peut utiliser `ifconfig -a` (outil **net-tools**, parfois à installer car déprécié au profit d’**iproute2**) ou mieux `ip addr show` (commande moderne `ip`). Ces commandes montrent si l’interface est **UP** ou **DOWN**, l’adresse IP assignée, et comptent les paquets émis/reçus ainsi que les erreurs (*RX/TX errors*). Une augmentation du compteur d’erreurs ou des **collisions** sur une interface Ethernet peut indiquer un problème de câble, de négociation de vitesse/duplex, ou de carte réseau. L’outil **ethtool** est précieux pour interroger une interface Ethernet : `ethtool eth0` affiche par exemple la vitesse (ex: *Speed: 1000Mb/s*), le mode duplex, l’état du lien (*Link detected: yes/no*), et peut montrer si l’auto-négociation a échoué. On peut aussi activer des statistiques plus détaillées (`ethtool -S eth0`) selon les pilotes, pour voir les compteurs d’erreurs bas niveau. Pour tester la connectivité, la commande de base est `ping`. Par exemple `ping -c4 8.8.8.8` envoie 4 paquets ICMP vers une adresse et mesure les temps de réponse. Aucun retour implique un problème de connectivité (pas d’accès à Internet dans cet exemple), tandis qu’un délai long ou de la perte de paquets (affichée en pourcentage) signale un souci de qualité de connexion. **Ping** vérifie la **reachabilité** d’une hôte sur le réseau IP et mesure le *round-trip time* des paquets ([ Linux network troubleshooting — Commands and examples | CodiLime](https://codilime.com/blog/linux-network-troubleshooting/#:~:text=ping%20The%20ping%20command%20tests,traversed%20layer%203)). Pour aller plus loin, **traceroute** (ou `mtr`) permet de diagnostiquer le chemin réseau et d’identifier où ça coince : `traceroute host.com` listera les routeurs traversés et les temps de transit, ce qui aide à voir si un nœud intermédiaire pose problème (paquets perdus ou forte latence à une étape). L’outil **mtr** combine ping et traceroute en continu, affichant l’évolution de la latence et de la perte à chaque nœud. Ces outils permettent de distinguer un problème local (câble, NIC) d’un problème sur le parcours réseau. Enfin, `netstat -an` (ou `ss -tulpn` plus moderne) affiche les connexions réseau et ports ouverts du serveur : cela peut être utile pour vérifier qu’un service écoute bien (sinon le problème est logiciel) ou pour repérer de nombreuses connexions dans des états inhabituels (par ex. beaucoup de connexions en *TIME_WAIT* pouvant indiquer un souci logiciel, ou en *SYN_SENT* indiquant des paquets sortants sans réponse). **Note:** Sur certaines distributions récentes, `netstat` n’est plus installé par défaut (remplacé par `ss` d’iproute2), il faut alors installer le paquet *net-tools* si on veut l’utiliser.

**Diagnostic du stockage :** Le sous-système de stockage requiert une attention particulière car une panne de disque peut entraîner une perte de données. Plusieurs outils permettent d’**inspecter les disques et systèmes de fichiers** :  
- `lsblk` liste les périphériques **bloc** (disques, SSD, clés USB, etc.) sous forme arborescente. Par exemple, il affichera `/dev/sda` avec ses partitions `/dev/sda1`, etc., ainsi que leur taille et éventuellement le point de montage. C’est utile pour vérifier qu’un disque est bien détecté par le système.  
- `fdisk -l` ou `gdisk -l` donnent le détail des **partitions** et de leur type sur chaque disque. On peut ainsi voir si une partition est manquante ou corrompue (par ex. *Partition table invalid*).  
- `df -h` indique l’**espace disque disponible** sur les systèmes de fichiers montés – un disque plein peut sembler être une “panne” (le système n’arrive plus à écrire, d’où erreurs d’E/S) mais c’est un problème logique facilement résolu en libérant de l’espace.  
- Pour le **diagnostic performance et I/O**, la suite sysstat fournit `iostat`. La commande `iostat -x 1 3` par exemple affichera, à intervalle d’1 seconde, les statistiques d’utilisation des disques : pour chaque device, le taux de transfert (kB/s), le nombre d’opérations par seconde (IOPS), et surtout le temps moyen d’attente (await) et pourcentage d’occupation du disque (%util). Un %util proche de 100% en permanence indique que le disque est saturé en I/O. Selon la distribution, *sysstat/iostat* peut être à installer (`apt install sysstat` sur Debian). **iostat** est conçu pour surveiller les statistiques d’entrées/sorties des périphériques et partitions ([iostat command in Linux with examples - GeeksforGeeks](https://www.geeksforgeeks.org/iostat-command-in-linux-with-examples/#:~:text=GeeksforGeeks%20www,statistics%20for%20devices%20and%20partitions)). On peut aussi utiliser `iotop` (à installer séparément, ex. `apt install iotop`), un utilitaire interactif qui liste les processus en cours consommant le plus d’I/O disque, ce qui aide à identifier quel service “fait ramer” un disque.  
- Enfin, pour vérifier l’**intégrité matérielle d’un disque**, on utilise **SMART** via l’outil `smartctl` (fourni par le paquet *smartmontools* sur la plupart des distributions). La commande `smartctl -a /dev/sdX` (X étant la lettre du disque, par ex. sda) affiche les informations SMART du disque : son **état de santé global** (PASS/FAIL), le compteur d’heures de fonctionnement, le nombre d’erreurs hardware, et une liste d’attributs tels que le nombre de **secteurs réalloués** (Reallocated Sector Count), de **secteurs en attente** (Pending Sector) etc. Par exemple, un output de `smartctl -a` pourrait montrer *Reallocated_Sector_Ct = 5* et *Current Pending Sector = 2*, signes que des secteurs ont été marqués défectueux et d’autres sont en attente de remapping – ceci alerte qu’un disque est en train de faillir. SMART indique aussi si le dernier autotest de disque a réussi ou renvoyé une erreur. Cet outil fournit donc une **vue d’ensemble de la santé du disque**, incluant le nombre d’erreurs recensées et si le test interne du disque est passé ou non ([S.M.A.R.T Data Reports - Evaluating Linux Storage Drive Health](https://www.hivelocity.net/kb/smart-data-reports-evaluating-linux-storage-drive-health/#:~:text=the%20S,sda1%2Csdb4%2C%20sdc7)). Sur Debian/Ubuntu, assurez-vous d’avoir activé SMART sur le disque (`smartctl -s on /dev/sdX` pour l’activer si ce n’est pas déjà le cas). Sur les SSD NVMe, on utilisera la commande équivalente `nvme smart-log` via l’outil *nvme-cli*.  
- Si SMART révèle des suspicions ou si l’on veut tester directement la surface du disque, on peut utiliser **badblocks**. Par exemple `badblocks -sv /dev/sdX` va scanner le disque en lecture pour détecter les blocs défectueux (le `-s` affiche la progression et le `-v` rend la sortie verbeuse). S’il trouve des bad blocks, il les listera à l’écran à la fin du scan. **Attention** : `badblocks` en mode non-destructif (par défaut) se contente de lire, ce qui est sûr pour les données, mais en mode **destructif** (option `-w` pour écrire) il efface tout – à n’utiliser qu’après sauvegarde et sur un disque hors service. Sur un système de fichiers monté, on privilégiera l’outil `e2fsck -c` (pour ext4) qui utilise badblocks en lecture et marque les blocs défectueux dans le système de fichiers. En pratique, si un disque signale des secteurs défectueux dans SMART ou badblocks, il est conseillé de le remplacer rapidement.

### 3. Exemples de travaux pratiques

Dans cette section, nous mettons en œuvre les outils précédemment mentionnés à travers des exemples concrets, en soulignant les éventuelles différences entre **Debian**, **Ubuntu**, **RHEL/CentOS** et **Arch Linux**.

#### Afficher les caractéristiques d’un matériel

Pour obtenir un **inventaire matériel détaillé du serveur**, on peut utiliser plusieurs utilitaires complémentaires :

- **`lshw`** (Liste hardware) – Cet outil offre un listing complet de la configuration matérielle. Par exemple, `sudo lshw -short` affiche un résumé des composants (carte mère, CPU, RAM, disques, interfaces réseau, etc.), tandis que `sudo lshw -class storage -class disk` ciblera uniquement les informations de stockage. La commande *lshw* scanne le matériel et fournit un maximum de détails sur chaque périphérique (adresse bus, IRQ, modules pilotes associés…). **Note:** Sous certaines distributions minimalistes (comme Arch), lshw n’est pas installé par défaut, il faut l’ajouter (*pacman -S lshw*). Sur RHEL, il est disponible via les dépôts EPEL. Sur Debian/Ubuntu, il suffit de `apt install lshw`. Lshw est réputé pour révéler *« tout »* sur votre matériel Linux ([lshw Command in Linux: Get Hardware Details](https://linuxhandbook.com/lshw-command/#:~:text=lshw%20Command%20in%20Linux%3A%20Get,Hardware%20Details)), ce qui est précieux lors d’un dépannage.

- **`hwinfo`** – Similaire à lshw, *hwinfo* (Hardware Info) est surtout utilisé sur openSUSE mais installable ailleurs (paquet *hwinfo*). `hwinfo` sans argument produit un rapport très verbeux de tout le matériel. On peut le filtrer par type, par ex. `hwinfo --cpu` ou `hwinfo --disk`. C’est un outil utile si lshw n’est pas disponible, mais il est moins courant sur Debian/Ubuntu par défaut.

- **`dmidecode`** – Cet utilitaire interroge la table DMI/SMBIOS fournie par le BIOS/UEFI pour extraire des informations sur le hardware. C’est très utile pour obtenir, par exemple, le numéro de série du châssis, la référence du serveur, la liste des barrettes RAM (avec numéros de série), la version du BIOS, etc., le tout sans ouvrir la machine. *Dmidecode* traduit la table DMI en un format lisible ([Linux tools: examining hardware in the terminal with dmidecode](https://www.redhat.com/en/blog/linux-tools-dmidecode#:~:text=The%20DMI%20table%20decoder%20is,using%20a%20few%20key%20flags)). Par exemple, `dmidecode -t memory` listera les informations sur la mémoire (chaque module avec sa capacité, fabricant, numéro de série, etc.), `dmidecode -t system` donnera le fabricant et le modèle du système, et `dmidecode -t bios` la version du BIOS. Sur Debian/Ubuntu et RHEL, dmidecode est généralement préinstallé ou dans les dépôts standard (paquet *dmidecode*). **Astuce:** l’option `-q` (quiet) réduit la verbosité et `-s` permet de demander un seul champ (par ex. `dmidecode -s system-serial-number` renvoie le numéro de série système). En résumé, *dmidecode* permet de récupérer des infos matérielles que même lshw/hwinfo n’affichent pas (comme les limites de RAM supportée, la présence d’ECC, etc. renseignées par le fabricant) ([Linux tools: examining hardware in the terminal with dmidecode](https://www.redhat.com/en/blog/linux-tools-dmidecode#:~:text=The%20DMI%20table%20decoder%20is,using%20a%20few%20key%20flags)).

- **`lspci`** – Pour lister les périphériques connectés sur le bus PCI/PCIe, on utilise lspci (fourni par le paquet *pciutils* sur toutes les distros). La commande `lspci` sans argument renvoie la liste des appareils PCI avec leur identifiant et une description. Cela inclut typiquement les contrôleurs SATA/RAID, les cartes réseau, les cartes graphiques, etc. Par exemple, on peut voir une ligne `02:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection`. L’option `-v` donne plus de détails (resources, IRQ, driver chargé ou non), et `-k` affiche le pilote kernel et le module utilisé pour chaque périphérique. *lspci* est l’outil de base pour vérifier qu’un matériel est détecté sur le bus et connaître son **identifiant PCI** (utilisé ensuite pour trouver un pilote si nécessaire). Par exemple, si une carte n’apparaît pas dans lspci, c’est qu’elle n’est pas du tout détectée (panne matérielle ou pas insérée correctement). **Astuce:** Utilisez `lspci -nn` pour voir également les identifiants numériques (vendor:device), ce qui aide à rechercher un pilote compatible. *Lspci* n’a pas besoin de privilèges root pour lire les informations PCI, un utilisateur normal peut l’exécuter ([System insights with command-line tools: dmidecode and lspci - Fedora Magazine](https://fedoramagazine.org/system-insights-command-line-dmidecode-lspci/#:~:text=lspci%3A%20Listing%20PCI%20devices)).

- **`lsusb`** – De même, lsusb (paquet *usbutils*) liste les périphériques USB connectés. Un simple `lsusb` affiche les bus USB et les périphériques avec leur VID:PID, par ex. `Bus 002 Device 005: ID 046d:c31c Logitech Keyboard`. C’est utile pour vérifier la présence d’un matériel USB (clé USB, disque externe, dongle, etc.) et son identification. Comme pour lspci, `lsusb -v` en root donne des informations détaillées (descripteurs, configurations). Sur un serveur, lsusb peut servir à identifier un onduleur USB, un dongle de licence, etc. *Lsusb* affiche les bus USB du système, les appareils qui y sont branchés et des infos comme l’ID fabricant et produit de chaque appareil ([System insights with command-line tools: lscpu and lsusb - Fedora Magazine](https://fedoramagazine.org/system-insights-with-command-line-tools-lscpu-and-lsusb/#:~:text=lsusb%20%E2%80%93%20Display%20USB%20Devices,Information)). Par exemple, sous Debian/Ubuntu lsusb est installé par défaut avec *usbutils*, sur certaines minimal (Arch minimal) il faut l’ajouter.

- **Compatibilité matériel-noyau (modules)** : Identifier un périphérique ne suffit pas, il faut aussi savoir si le noyau Linux l’a pris en charge via un **module** (pilote). La commande `lsmod` liste tous les modules du noyau actuellement chargés. On peut filtrer, par ex. `lsmod | grep e1000` pour voir si le pilote e1000 (carte Intel) est chargé. Si un matériel listé par lspci n’a pas de module correspondant, c’est qu’il n’est pas géré (pilote manquant ou non chargé). Dans ce cas, on peut tenter de le charger manuellement (`modprobe nom_module`). Pour en savoir plus sur un module, utilisez `modinfo nom_module`. Par exemple, `modinfo e1000e` donnera la version du pilote Intel PRO/1000, l’auteur, et crucialement la liste des alias PCI ID supportés (lignes commençant par **alias: pci:**…). Cela permet de vérifier si le matériel identifié par lspci (via son Vendor:Device ID) est pris en charge par ce module. En dépannage, si un périphérique ne fonctionne pas, `modinfo` peut aussi révéler des options configurables du module ou des dépendances. En résumé, **lsmod** permet de voir les modules actifs (pour détecter une absence de pilote), et **modinfo** d’obtenir des détails sur un module (pour vérifier s’il correspond au matériel) ([Using The lsmod and modinfo Commands in Linux |
Linode Docs](https://www.linode.com/docs/guides/lsmod-and-modinfo-commands-in-linux/#:~:text=It%20is%20important%20to%20inspect,your%20Linux%20system%E2%80%99s%20kernel%20modules)). Sous Debian/Ubuntu, ces commandes sont natives (paquet *kmod*); idem sur RHEL et Arch. Notons que sur Ubuntu/Debian, la présence du module est souvent gérée par udev qui le charge automatiquement si le matériel est détecté. Si ce n’est pas le cas, on peut avoir à le charger à la main ou installer le paquet kernel approprié (par ex. ajouter un *backport kernel* ou un pilote propriétaire). Sur RHEL, il faut parfois installer un pilote tiers (via DKMS) si le matériel n’est pas supporté nativement par le noyau de base.

En combinant ces outils, on peut dresser un portrait complet du matériel du serveur et de sa prise en charge par Linux. Par exemple, face à un nouveau serveur, un administrateur Debian pourrait faire : `lshw -short` pour la liste globale, `lspci -nn` pour relever tous les IDs de matériels, puis vérifier pour chacun s’il y a un module (`lsmod`/`modinfo`). Sur un système Red Hat, on ferait de même (avec lshw installé via EPEL si nécessaire). Sur Arch, on utiliserait `lspci`/`lsusb` directement et peut-être installer *inxi* ou *lshw* pour plus de détails. Cette étape d’inventaire permet de s’assurer que **rien ne manque** (par ex. toutes les RAM sont vues, toutes les cartes réseau listées) avant de passer au diagnostic des incidents.

#### Identifier les incidents associés

Dans un scénario de **panne avérée**, voici comment appliquer concrètement les outils de diagnostic pour identifier l’origine du problème :

- **Analyse des logs système en cas d’erreur** : Supposons qu’un serveur rencontre des plantages ou comportements étranges. La première chose à faire est de consulter les journaux. Sous Debian/Ubuntu on regardera `tail -n50 /var/log/syslog` (ou `dmesg` pour les messages noyau récents), sous RHEL `tail -n50 /var/log/messages`, ou directement `journalctl -xe` pour les dernières entrées. On peut chercher des indices d’erreur : `dmesg | grep -i error` fera ressortir les erreurs noyau (exemple de sortie : *`ata3.00: failed command: READ DMA`*, *`I/O error, dev sda, sector 123456`* indiquant une erreur de lecture disque). De même, `journalctl -p err -b` listera toutes les erreurs (`-p err` pour priorité erreur) du boot courant. Ces messages vous orientent : une erreur ATA ou SCSI suggère un problème de disque, une trace d’**OOM Killer** (Out-Of-Memory) indique un manque de RAM à un moment donné (processus tués car la mémoire était épuisée), un message *“CPU throttling”* ou *“temperature above threshold”* indique une surchauffe CPU, etc. Par exemple, un extrait de log pourrait montrer : `kernel: Out of memory: Kill process 1234 (apache2) score 500` ce qui indique que le système a manqué de RAM et a dû tuer le processus en question ([How To Troubleshoot Linux Server Memory Issues - UpCloud](https://upcloud.com/resources/tutorials/troubleshoot-linux-memory-issues#:~:text=Suddenly%20killed%20tasks%20are%20often,log%20files%20stored%20at%20%2Fvar%2Flog)) ([How To Troubleshoot Linux Server Memory Issues - UpCloud](https://upcloud.com/resources/tutorials/troubleshoot-linux-memory-issues#:~:text=kernel%3A%20Out%20of%20memory%3A%20Kill,score%20511%20or%20sacrifice%20child)). Dans ce cas, le matériel (RAM) n’est pas forcément défaillant, mais insuffisant pour la charge – on ajustera la RAM ou la configuration logicielle. Autre exemple, une erreur réseau typique dans les logs : `e1000e: eth0: Detected Hardware Unit Hang` suivi de réinitialisation de l’interface, cela peut indiquer un problème du pilote ou de la carte réseau elle-même. En scrutant les logs, on peut donc souvent **associer un symptôme à un composant matériel**. Un administrateur devra ensuite décider si cela relève d’un remplacement matériel (ex : disque physiquement endommagé avec erreurs I/O) ou d’un correctif logiciel (ex : mise à jour d’un pilote buggué causant le *hang* réseau).

- **Disque dur en panne** : Prenons le cas fréquent d’un disque dont on suspecte la panne (messages d’erreurs d’E/S dans les logs, performances en chute, etc.). La procédure de diagnostic consistera à :  
  1. **Vérifier les données SMART** : `smartctl -a /dev/sdX` (X étant le disque concerné). On examine la ligne **SMART overall-health** – si elle indique FAILING or PASSED. Si c’est *FAILING* ou *SMART Error Log* montre des erreurs, c’est un très mauvais signe. On regarde les attributs : un nombre non nul et en hausse de *Reallocated_Sector_Ct* ou de *Current_Pending_Sector* confirme que le disque a des secteurs défectueux qu’il tente de réallouer ([S.M.A.R.T Data Reports - Evaluating Linux Storage Drive Health](https://www.hivelocity.net/kb/smart-data-reports-evaluating-linux-storage-drive-health/#:~:text=the%20S,sda1%2Csdb4%2C%20sdc7)). Par exemple, *Reallocated_Sector_Ct = 50* signifie que 50 secteurs ont été remplacés par des secteurs de secours – même si le disque est encore fonctionnel, cela indique qu’il a subi des erreurs et qu’il est potentiellement en fin de vie. De même, des valeurs élevées de *UDMA_CRC_Error_Count* pourraient suggérer un câble SATA défectueux plutôt que le disque lui-même. Comparer ces valeurs avec des références en ligne ou la documentation du fabricant peut aider à conclure.  
  2. **Tester la surface du disque** : en complément, surtout si SMART n’indique rien de flagrant mais qu’on a des doutes, on lance `badblocks`. Par exemple, `badblocks -sv /dev/sdX > badblocks.log` pour scanner tout le disque et enregistrer les blocs illisibles. Si le programme répertorie des numéros de blocs en erreur, le disque a des secteurs irréparables. À ce stade, sur un serveur en production, s’il s’agit d’un disque membre d’un RAID, on pourrait le marquer défaillant et le remplacer, ou s’il n’y a pas de RAID, planifier une copie des données vers un nouveau disque avant qu’il ne lâche complètement. Sur Debian/Ubuntu, *smartmontools* et *badblocks* (dans *e2fsprogs*) sont disponibles par défaut ou faciles à installer. Sur RHEL, *smartmontools* est également disponible, et *badblocks* fait partie des utilitaires e2fsprogs de base. Sur Arch, idem, ces outils sont présents (smartmontools, e2fsprogs).  
  3. **Interprétation** : Si SMART et badblocks ne révèlent rien mais que le disque continue à poser problème (par exemple des *timeouts* dans `dmesg` : *`blk_update_request: I/O error, dev sda, sector X`*), il peut s’agir d’un problème du **contrôleur** (carte RAID, câble mal branché, etc.) ou du **système de fichiers** (corruption logique). On pourrait alors lancer un fsck du système de fichiers (`umount` puis `fsck.ext4 -f /dev/sdXn`). Mais généralement, des erreurs d’E/S répétées sont signe d’une panne matérielle. La différence Debian/RHEL ici est minime, si ce n’est que RHEL en entreprise utilisera souvent des solutions comme *smartd* (daemon SMART) pour remonter automatiquement les alertes SMART, chose que l’on peut aussi activer sur Debian en configurant `/etc/smartd.conf`.

- **Surcharge CPU** : Considérons un serveur où *top* indique 100% CPU en permanence, ou simplement des ralentissements inexpliqués. Pour diagnostiquer :  
  1. On exécute `top` ou `htop` pour voir **quel processus** monopolise le CPU. Par exemple, top peut montrer un processus particulier à 250% (sur 4 cœurs) – peut-être un script en boucle infinie ou un service coincé. À l’aide de `ps aux --sort=-%cpu | head -5`, on obtient la liste des 5 processus les plus gourmands en CPU. Supposons qu’on voie `mysqld` ou `java` en haut constamment à ~90% : cela oriente vers une charge applicative élevée (requêtes SQL lourdes, etc.) plutôt qu’un problème matériel. En revanche, si aucun processus utilisateur n’explique la charge (cas rarissime : CPU utilisé en kernel uniquement, iowait très élevé, etc.), on doit envisager un problème plus bas niveau.  
  2. On vérifie avec `mpstat -P ALL 5 5` l’utilisation par cœur. Si un seul cœur est saturé, c’est peut-être un thread unique bloqué. Si *tous* les cœurs sont saturés en sys (mode kernel) avec un iowait élevé, cela peut indiquer que le CPU attend le disque (donc le bottleneck serait plutôt le disque que le CPU). L’indicateur *%iowait* dans *mpstat* ou *top* est crucial : un iowait haut signifie que le CPU est inoccupé en attente d’I/O – ce n’est pas le CPU le problème mais souvent le disque lent.  
  3. On peut aussi regarder la température CPU (via `sensors`) car un CPU en surchauffe va réduire sa fréquence (*throttling*) ce qui donne l’impression d’une machine lente même avec peu de charge. Si les températures sont élevées (près du maximum TJunction), on doit vérifier le refroidissement (ventilateurs, pâte thermique) plutôt que blâmer la charge logicielle.  
  4. Sur Debian/Ubuntu, ces outils (top/htop, sysstat) sont facilement accessibles. Sur RHEL, *top* est intégré, *htop* à installer via EPEL, *sysstat/mpstat* souvent installé de base sur Red Hat pour la collecte de performance (par exemple via *sar*). Sur Arch, *htop* et *sysstat* sont disponibles également.  
  5. Si la conclusion est qu’un processus particulier cause la surcharge (ex: un bug dans une application), c’est un problème logiciel. Si la charge est due à un facteur externe (comme iowait dû à un disque mourant), on revient au diagnostic stockage. Enfin, si le CPU lui-même est fautif (rare, mais par ex. une instruction particulière cause un kernel panic en raison d’un bug microcode), on verrait des **kernel panic** ou MCE dans les logs, ce qui est un cas très spécifique nécessitant mise à jour microcode ou remplacement CPU.

- **Erreurs réseau** : Imaginez un serveur qui perd la connectivité par intermittence. On peut procéder ainsi :  
  1. **Pings successifs** – `ping -c4 gateway` (la passerelle) pour voir si le lien local fonctionne, puis `ping -c4 8.8.8.8` pour tester l’accès Internet. Si la passerelle est injoignable (*Destination Host Unreachable*), le problème est local (interface, câble, switch). Si la passerelle répond mais pas Internet, c’est potentiellement un problème extérieur (ou DNS).  
  2. **MTR** – Exécuter `mtr --report google.com` pendant 30 secondes pour voir à quel **saut** les pertes ou la latence apparaissent. Si dès le premier saut (la passerelle) il y a de la perte, on suspecte la connexion serveur <-> switch. Par exemple, un *mtr* ou *traceroute* qui indique des temps de réponse instables dès le routeur local ou des *packet loss* locaux orientent vers un problème de câble ou de carte.  
  3. **Logs kernel** – `dmesg` peut montrer des messages comme *“link down”* / *“link up”* fréquents sur l’interface réseau, ce qui pourrait signifier que l’interface se réinitialise (perte du signal, peut-être à cause d’une carte défaillante ou d’un câble qui bouge). Un message *“NETDEV WATCHDOG: eth0: transmit queue timed out”* est un signe classique d’un problème sérieux sur l’interface (soit un bug du driver, soit la carte qui ne répond plus aux interruptions).  
  4. **Vérifs matérielles** – Avec `ethtool eth0`, on vérifie si l’**Auto-Negotiation** a bien négocié la bonne vitesse/duplex. Un problème courant est un mismatch duplex (par ex. l’interface en 1000Mb Full mais le switch en half-duplex ou mal négocié). Bien que rare de nos jours, cela peut causer de gros ralentissements et des erreurs. Ethtool indiquera aussi si le **Link detected** est *yes* et des compteurs d’erreurs bas niveau.  
  5. **Netstat** – Si la connectivité revient mais certaines applications n’arrivent toujours pas à communiquer, on peut vérifier les sockets avec `netstat -an`. Par exemple, si un service web est inaccessible, `netstat -an | grep LISTEN` permet de voir si le port 80/443 est à l’écoute. Si ce n’est pas le cas, le souci n’est pas matériel mais le service qui a crashé (à redémarrer). Si tout est bien à l’écoute, on peut inspecter `netstat -s` qui donne des statistiques par protocole (TCP retransmissions, UDP erreurs, etc.). Un taux élevé de **retransmissions TCP** peut suggérer un réseau instable.  
  6. **Comparaison distro** – Sur Debian/Ubuntu, `ifconfig` et `netstat` sont disponibles via *net-tools* (à installer manuellement sur les versions récentes car ces commandes sont obsolètes). On privilégie `ip` et `ss` (`ip a`, `ss -tulpn`) équivalents modernes. Sur RHEL, *net-tools* est également optionnel, Red Hat encourage *ip/ss*. Sur Arch, net-tools n’est pas par défaut non plus. Mais dans le contexte de dépannage, l’important est d’utiliser les commandes qu’on a sous la main pour isoler la cause. Si après toutes ces vérifications on suspecte fortement le matériel (ex: l’interface tombe dès qu’on dépasse un certain débit), on essaiera idéalement de changer le câble réseau, tester sur un autre port du switch, voire insérer une autre carte réseau si possible. 

En résumé, le **dépannage matériel sous Linux** requiert d’observer les bons indicateurs (logs, capteurs, statuts des périphériques) et d’utiliser les outils adéquats pour chaque sous-système. Debian et Ubuntu fournissent une riche panoplie d’outils en standard ou via APT, Red Hat offre des équivalents (souvent à installer via YUM/DNF si non inclus dans l’installation minimale), et Arch Linux aussi via pacman. Les commandes elles-mêmes sont généralement identiques d’une distribution à l’autre – ce qui change surtout, ce sont les chemins de logs par défaut et les gestionnaires de paquets pour installer les utilitaires manquants. Avec la pratique, un administrateur saura interpréter, par exemple, qu’une série d’erreurs *“ATA bus error”* dans `dmesg` couplée à des *“I/O error”* sur un disque monté indique un disque en panne (ou un câble SATA à vérifier), ou qu’un *kernel panic* avec une trace pointant vers la mémoire peut justifier un passage de Memtest86+. Grâce aux outils et méthodes décrits, on peut identifier la plupart des pannes matérielles sur un serveur Linux et agir en conséquence : remplacer le composant défectueux ou corriger le problème (par ex. mise à jour d’un firmware/BIOS, ajout de refroidissement, etc.), tout en distinguant ce qui relève du matériel ou du logiciel à chaque étape du diagnostic. ([How to troubleshoot hardware problems in Linux](https://www.dedoimedo.com/computers/linux-hardware-troubleshooting.html#:~:text=Some%20kind%20of%20errors%20may,gaming%C2%A0desktop%20case%20ground%20wiring%20issues)) ([Linux Logging Basics - The Ultimate Guide To Logging](https://www.loggly.com/ultimate-guide/linux-logging-basics/#:~:text=,and%20output%20from%20pluggable%20authentication)) ([Linux tools: examining hardware in the terminal with dmidecode](https://www.redhat.com/en/blog/linux-tools-dmidecode#:~:text=The%20DMI%20table%20decoder%20is,using%20a%20few%20key%20flags)) ([System insights with command-line tools: dmidecode and lspci - Fedora Magazine](https://fedoramagazine.org/system-insights-command-line-dmidecode-lspci/#:~:text=lspci%3A%20Listing%20PCI%20devices)) ([System insights with command-line tools: lscpu and lsusb - Fedora Magazine](https://fedoramagazine.org/system-insights-with-command-line-tools-lscpu-and-lsusb/#:~:text=lsusb%20%E2%80%93%20Display%20USB%20Devices,Information)) ([Using The lsmod and modinfo Commands in Linux |
Linode Docs](https://www.linode.com/docs/guides/lsmod-and-modinfo-commands-in-linux/#:~:text=It%20is%20important%20to%20inspect,your%20Linux%20system%E2%80%99s%20kernel%20modules)) ([S.M.A.R.T Data Reports - Evaluating Linux Storage Drive Health](https://www.hivelocity.net/kb/smart-data-reports-evaluating-linux-storage-drive-health/#:~:text=the%20S,sda1%2Csdb4%2C%20sdc7))